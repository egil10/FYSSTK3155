{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9db2852",
   "metadata": {},
   "source": [
    "### Part b): Writing your own Neural Network code\n",
    "\n",
    "Your aim now, and this is the central part of this project, is to\n",
    "write your own FFNN code implementing the back\n",
    "propagation algorithm discussed in the lecture slides from week 41 at <https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/week41.html> and week 42 at <https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/week42.html>.\n",
    "\n",
    "We will focus on a regression problem first, using the one-dimensional Runge function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79855f",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\frac{1}{1+25x^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4111ef",
   "metadata": {},
   "source": [
    "from project 1.\n",
    "\n",
    "Use only the mean-squared error as cost function (no regularization terms) and \n",
    "write an FFNN code for a regression problem with a flexible number of hidden\n",
    "layers and nodes using only the Sigmoid function as activation function for\n",
    "the hidden layers. Initialize the weights using a normal\n",
    "distribution. How would you initialize the biases? And which\n",
    "activation function would you select for the final output layer?\n",
    "And how would you set up your design/feature matrix? Hint: does it have to represent a polynomial approximation as you did in project 1? \n",
    "\n",
    "Train your network and compare the results with those from your OLS\n",
    "regression code from project 1 using the one-dimensional Runge\n",
    "function.  When comparing your neural network code with the OLS\n",
    "results from project 1, use the same data sets which gave you the best\n",
    "MSE score. Moreover, use the polynomial order from project 1 that gave you the\n",
    "best result.  Compare these results with your neural network with one\n",
    "and two hidden layers using $50$ and $100$ hidden nodes, respectively.\n",
    "\n",
    "Comment your results and give a critical discussion of the results\n",
    "obtained with the OLS code from project 1 and your own neural network\n",
    "code.  Make an analysis of the learning rates employed to find the\n",
    "optimal MSE score. Test both stochastic gradient descent\n",
    "with RMSprop and ADAM and plain gradient descent with different\n",
    "learning rates.\n",
    "\n",
    "You should, as you did in project 1, scale your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc76d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2cf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Our own code ---\n",
    "from neural_network import NeuralNetwork\n",
    "from optimizers import SGD, RMSprop, Adam\n",
    "from losses import mse, mse_deriv\n",
    "from activations import sigmoid, sigmoid_deriv, linear, linear_deriv, relu, relu_deriv\n",
    "from prepare_data import prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a2cf6",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d66f2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m seed = \u001b[32m6114\u001b[39m\n\u001b[32m      2\u001b[39m n_datapoints = \u001b[32m100\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m x, y, x_train, x_test, y_train, y_test = \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_datapoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m scaler_x = StandardScaler()\n\u001b[32m      6\u001b[39m x_train = scaler_x.fit_transform(x_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/FYSSTK3155/PROJECT 2/Code/prepare_data.py:31\u001b[39m, in \u001b[36mprepare_data\u001b[39m\u001b[34m(n, noise_scale, noise)\u001b[39m\n\u001b[32m     28\u001b[39m noise = rng.normal(loc=\u001b[32m0.0\u001b[39m, scale=noise_scale, size=x.shape)\n\u001b[32m     29\u001b[39m y_noisy = y + noise\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m noise: \n\u001b[32m     32\u001b[39m     x_train, x_test, y_train, y_test = train_test_split(x, y_noisy, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m6114\u001b[39m)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x, y, x_train, x_test, y_train, y_test\n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "seed = 6114\n",
    "n_datapoints = 100\n",
    "x, y, x_train, x_test, y_train, y_test = prepare_data(n=n_datapoints)\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "x_train = scaler_x.fit_transform(x_train)\n",
    "x_test = scaler_x.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506bd1a1",
   "metadata": {},
   "source": [
    "## Building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede40c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output_sizes = [5,1]\n",
    "activation_funcs = [sigmoid, linear]\n",
    "activation_ders = [sigmoid_deriv, linear_deriv]\n",
    "cost_func = mse\n",
    "cost_func_der = mse_deriv\n",
    "\n",
    "nn = NeuralNetwork(network_input_size=1, \n",
    "                   layer_output_sizes=layer_output_sizes, \n",
    "                   activation_funcs=activation_funcs,\n",
    "                   activation_ders=activation_ders,\n",
    "                   cost_fun=cost_func,\n",
    "                   cost_der=cost_func_der)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d287352",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb82d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "optimizer = Adam(lr=lr)\n",
    "history = nn.fit(x_train, \n",
    "                 y_train,\n",
    "                 epochs=50,\n",
    "                 batch_size=10,\n",
    "                 optimizer=optimizer,\n",
    "                 X_val = x_test,\n",
    "                 Y_val = y_test,\n",
    "                 log_every=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYS-STK3155",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
