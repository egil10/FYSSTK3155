{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3339857",
   "metadata": {},
   "source": [
    "from project 1.\n",
    "\n",
    "Use only the mean-squared error as cost function (no regularization terms) and \n",
    "write an FFNN code for a regression problem with a flexible number of hidden\n",
    "layers and nodes using only the Sigmoid function as activation function for\n",
    "the hidden layers. Initialize the weights using a normal\n",
    "distribution. How would you initialize the biases? And which\n",
    "activation function would you select for the final output layer?\n",
    "And how would you set up your design/feature matrix? Hint: does it have to represent a polynomial approximation as you did in project 1? \n",
    "\n",
    "Train your network and compare the results with those from your OLS\n",
    "regression code from project 1 using the one-dimensional Runge\n",
    "function.  When comparing your neural network code with the OLS\n",
    "results from project 1, use the same data sets which gave you the best\n",
    "MSE score. Moreover, use the polynomial order from project 1 that gave you the\n",
    "best result.  Compare these results with your neural network with one\n",
    "and two hidden layers using $50$ and $100$ hidden nodes, respectively.\n",
    "\n",
    "Comment your results and give a critical discussion of the results\n",
    "obtained with the OLS code from project 1 and your own neural network\n",
    "code.  Make an analysis of the learning rates employed to find the\n",
    "optimal MSE score. Test both stochastic gradient descent\n",
    "with RMSprop and ADAM and plain gradient descent with different\n",
    "learning rates.\n",
    "\n",
    "You should, as you did in project 1, scale your data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
