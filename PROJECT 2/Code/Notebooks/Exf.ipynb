{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230db1ec",
   "metadata": {},
   "source": [
    "### Part f): Classification  analysis using neural networks\n",
    "\n",
    "With a well-written code it should now be easy to change the\n",
    "activation function for the output layer.\n",
    "\n",
    "Here we will change the cost function for our neural network code\n",
    "developed in parts b), d) and e) in order to perform a classification\n",
    "analysis.  The classification problem we will study is the multiclass\n",
    "MNIST problem, see the description of the full data set at\n",
    "<https://www.kaggle.com/datasets/hojjatk/mnist-dataset>. We will use the Softmax cross entropy function discussed in a). \n",
    "The MNIST data set discussed in the lecture notes from week 42 is a downscaled variant of the full dataset. \n",
    "\n",
    "Feel free to suggest other data sets. If you find the classic MNIST data set somewhat limited, feel free to try the  \n",
    "MNIST-Fashion data set at for example <https://www.kaggle.com/datasets/zalando-research/fashionmnist>.\n",
    "\n",
    "To set up the data set, the following python programs may be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8891875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "from Implementations.activations import relu, relu_deriv, linear, linear_deriv\n",
    "from Implementations.losses import cross_entropy_with_logits, cross_entropy_with_logits_deriv\n",
    "from Implementations.optimizers import Adam\n",
    "from Implementations.neural_network import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08dd556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_from_logits(logits, y_true):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    labels = np.argmax(y_true, axis=1)\n",
    "    return np.mean(preds == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc82dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "fashion_train = datasets.FashionMNIST(root=\"./data\", train=True, download=True,transform=transform)\n",
    "fashion_test = datasets.FashionMNIST(root=\"./data\", train=False, download=True,transform=transform)\n",
    "\n",
    "X_train = fashion_train.data.numpy().astype(np.float64) / 255.0\n",
    "X_test = fashion_test.data.numpy().astype(np.float64) / 255.0\n",
    "\n",
    "y_train = fashion_train.targets.numpy().astype(np.int64)\n",
    "y_test = fashion_test.targets.numpy().astype(np.int64) \n",
    "\n",
    "X_train = X_train.reshape(-1, 28*28) #images are 28*28 pixels\n",
    "X_test = X_test.reshape(-1, 28*28)\n",
    "\n",
    "# One hot encoding \n",
    "Y_train = np.zeros((y_train.size, 10))\n",
    "Y_train[np.arange(y_train.size), y_train] = 1.0\n",
    "\n",
    "Y_test = np.zeros((y_test.size, 10))\n",
    "Y_test[np.arange(y_test.size), y_test] = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e7566c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | train: 0.555700\n",
      "Epoch   2 | train: 0.446539\n",
      "Epoch   3 | train: 0.404633\n",
      "Epoch   4 | train: 0.377808\n",
      "Epoch   5 | train: 0.356745\n",
      "Epoch   6 | train: 0.359595\n",
      "Epoch   7 | train: 0.325346\n",
      "Epoch   8 | train: 0.302108\n",
      "Epoch   9 | train: 0.286780\n",
      "Epoch  10 | train: 0.288042\n",
      "Epoch  11 | train: 0.287692\n",
      "Epoch  12 | train: 0.270885\n",
      "Epoch  13 | train: 0.271304\n",
      "Epoch  14 | train: 0.272062\n",
      "Epoch  15 | train: 0.258025\n",
      "Epoch  16 | train: 0.243352\n",
      "Epoch  17 | train: 0.242686\n",
      "Epoch  18 | train: 0.229097\n",
      "Epoch  19 | train: 0.223139\n",
      "Epoch  20 | train: 0.212646\n",
      "Epoch  21 | train: 0.209677\n",
      "Epoch  22 | train: 0.202467\n",
      "Epoch  23 | train: 0.211456\n",
      "Epoch  24 | train: 0.189287\n",
      "Epoch  25 | train: 0.189279\n",
      "Epoch  26 | train: 0.192431\n",
      "Epoch  27 | train: 0.184544\n",
      "Epoch  28 | train: 0.183938\n",
      "Epoch  29 | train: 0.175303\n",
      "Epoch  30 | train: 0.188206\n",
      "Epoch  31 | train: 0.167668\n",
      "Epoch  32 | train: 0.170759\n",
      "Epoch  33 | train: 0.159406\n",
      "Epoch  34 | train: 0.151235\n",
      "Epoch  35 | train: 0.171708\n",
      "Epoch  36 | train: 0.146014\n",
      "Epoch  37 | train: 0.138933\n",
      "Epoch  38 | train: 0.151197\n",
      "Epoch  39 | train: 0.145644\n",
      "Epoch  40 | train: 0.146541\n",
      "Epoch  41 | train: 0.128149\n",
      "Epoch  42 | train: 0.124529\n",
      "Epoch  43 | train: 0.126517\n",
      "Epoch  44 | train: 0.121634\n",
      "Epoch  45 | train: 0.124648\n",
      "Epoch  46 | train: 0.121593\n",
      "Epoch  47 | train: 0.108278\n",
      "Epoch  48 | train: 0.106734\n",
      "Epoch  49 | train: 0.125660\n",
      "Epoch  50 | train: 0.120904\n",
      "Epoch  51 | train: 0.105200\n",
      "Epoch  52 | train: 0.099562\n",
      "Epoch  53 | train: 0.095102\n",
      "Epoch  54 | train: 0.101308\n",
      "Epoch  55 | train: 0.106380\n",
      "Epoch  56 | train: 0.096025\n",
      "Epoch  57 | train: 0.112343\n",
      "Epoch  58 | train: 0.089862\n",
      "Epoch  59 | train: 0.082987\n",
      "Epoch  60 | train: 0.085706\n",
      "Epoch  61 | train: 0.092656\n",
      "Epoch  62 | train: 0.081369\n",
      "Epoch  63 | train: 0.090196\n",
      "Epoch  64 | train: 0.082805\n",
      "Epoch  65 | train: 0.079803\n",
      "Epoch  66 | train: 0.076001\n",
      "Epoch  67 | train: 0.073023\n",
      "Epoch  68 | train: 0.102115\n",
      "Epoch  69 | train: 0.093374\n",
      "Epoch  70 | train: 0.074344\n",
      "Epoch  71 | train: 0.081627\n",
      "Epoch  72 | train: 0.071661\n",
      "Epoch  73 | train: 0.084104\n",
      "Epoch  74 | train: 0.072758\n",
      "Epoch  75 | train: 0.077512\n",
      "Epoch  76 | train: 0.083739\n",
      "Epoch  77 | train: 0.067974\n",
      "Epoch  78 | train: 0.079997\n",
      "Epoch  79 | train: 0.060096\n",
      "Epoch  80 | train: 0.067507\n",
      "Epoch  81 | train: 0.059189\n",
      "Epoch  82 | train: 0.069808\n",
      "Epoch  83 | train: 0.123486\n",
      "Epoch  84 | train: 0.071879\n",
      "Epoch  85 | train: 0.058664\n",
      "Epoch  86 | train: 0.068765\n",
      "Epoch  87 | train: 0.064372\n",
      "Epoch  88 | train: 0.064915\n",
      "Epoch  89 | train: 0.072752\n",
      "Epoch  90 | train: 0.076161\n",
      "Epoch  91 | train: 0.043621\n",
      "Epoch  92 | train: 0.084971\n",
      "Epoch  93 | train: 0.065024\n",
      "Epoch  94 | train: 0.059553\n",
      "Epoch  95 | train: 0.038177\n",
      "Epoch  96 | train: 0.071718\n",
      "Epoch  97 | train: 0.048487\n",
      "Epoch  98 | train: 0.044803\n",
      "Epoch  99 | train: 0.037578\n",
      "Epoch 100 | train: 0.044378\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [128, 64, 10]\n",
    "activation_funcs = [relu, relu, linear]\n",
    "activation_ders  = [relu_deriv, relu_deriv, linear_deriv]\n",
    "\n",
    "net = NeuralNetwork(\n",
    "    network_input_size=X_train.shape[1],\n",
    "    layer_output_sizes=layer_sizes,\n",
    "    activation_funcs=activation_funcs,\n",
    "    activation_ders=activation_ders,\n",
    "    cost_fun=cross_entropy_with_logits,\n",
    "    cost_der=cross_entropy_with_logits_deriv,\n",
    "    seed=6114,\n",
    "    l2_lambda=1e-4\n",
    ")\n",
    "\n",
    "optimizer = Adam(lr=1e-3)\n",
    "\n",
    "history = net.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    optimizer=optimizer,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f58a76",
   "metadata": {},
   "source": [
    "To measure the performance of our classification problem we will use the\n",
    "so-called *accuracy* score.  The accuracy is as you would expect just\n",
    "the number of correctly guessed targets $t_i$ divided by the total\n",
    "number of targets, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c7ae7a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Accuracy} = \\frac{\\sum_{i=1}^n I(t_i = y_i)}{n} ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b7557",
   "metadata": {},
   "source": [
    "where $I$ is the indicator function, $1$ if $t_i = y_i$ and $0$\n",
    "otherwise if we have a binary classification problem. Here $t_i$\n",
    "represents the target and $y_i$ the outputs of your FFNN code and $n$ is simply the number of targets $t_i$.\n",
    "\n",
    "Discuss your results and give a critical analysis of the various parameters, including hyper-parameters like the learning rates and the regularization parameter $\\lambda$, various activation functions, number of hidden layers and nodes and activation functions.  \n",
    "\n",
    "Again, we strongly recommend that you compare your own neural Network\n",
    "code for classification and pertinent results against a similar code using **Scikit-Learn**  or **tensorflow/keras** or **pytorch**.\n",
    "\n",
    "If you have time, you can use the functionality of **scikit-learn** and compare your neural network results with those from Logistic regression. This is optional.\n",
    "The weblink  here <https://medium.com/ai-in-plain-english/comparison-between-logistic-regression-and-neural-networks-in-classifying-digits-dc5e85cd93c3>compares logistic regression and FFNN using the so-called MNIST data set. You may find several useful hints and ideas from this article. Your neural network code can implement the equivalent of logistic regression by simply setting the number of hidden layers to zero and keeping just the input and the output layers. \n",
    "\n",
    "If you wish to compare with say Logisti Regression from **scikit-learn**, the following code uses the above data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Initialize the model\n",
    "model = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=1000, random_state=42)\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYS-STK3155",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
